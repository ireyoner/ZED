{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Raport z zadania 'Uczenie Maszynowe'\n",
    "####Autorzy:\n",
    "Michał Bartecki 106518 \n",
    "\n",
    "Marcin Błaszyk  106616\n",
    "###Biblioteki i przygotowanie do pracy\n",
    "\n",
    "W celu przygotowania do pracy należy umieścić dane wejściowe w katalogu data oraz zainstalować wymagane biblioteki.\n",
    "\n",
    "W projekcie korzystaliśmy z python'a w wersji 3.5.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "#import joblib\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report, recall_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "NA_FILL = 0# -999999999999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Wczytanie i oczyszczenie danych\n",
    "Na początku przeprowadziliśmy wczytanie i oczyszczenie danych uczących.\n",
    "Istotne jest by pamiętać o ustawieniu wektora na_values=[\"nan\"] oraz zastosowaniu argumentu keep_default_na =False. W przeciwnym wypadku biblioteka pandas zamieni wszystkie wystąpienia klasy NA na wartości puste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    lebels_to_delete =[ 'DA','DC','DT','DU','DG','DI','UNK','UNX','UNL','PR','PD','Y1','EU','N','15P','UQ','PX4','NAN']\n",
    "\n",
    "    df_out = df[~df['res_name'].isin(lebels_to_delete)]\n",
    "\n",
    "    #unikalne pary\n",
    "    df_out = df_out.drop_duplicates(subset=['pdb_code','res_name'],keep='first')\n",
    "\n",
    "    #minimum 5 wystapien\n",
    "    df_out = df_out.groupby('res_name').filter(lambda x: len(x) >= 5)\n",
    "\n",
    "    ##Usuwanie niepotrzebnych kolumn\n",
    "    #local jakie chcemy zatrzymac\n",
    "    keep = ['local_volume', 'local_electrons', 'local_mean', 'local_std', 'local_min', 'local_max', 'local_skewness', 'local_parts']\n",
    "    columns_to_drop = list(df_out.columns[pd.Series(df_out.columns).str.startswith('local') | pd.Series(df_out.columns).str.startswith('dict')].difference(keep))\n",
    "    df_out = df_out.drop(columns_to_drop,axis=1)\n",
    "    columns_to_drop = ['title', 'pdb_code', 'res_id', 'chain_id','fo_col', 'fc_col', 'weight_col', 'grid_space', 'solvent_radius', 'solvent_opening_radius', 'part_step_FoFc_std_min', 'part_step_FoFc_std_max', 'part_step_FoFc_std_step']\n",
    "    df_out = df_out.drop(columns_to_drop,axis=1)\n",
    "\n",
    "    df_out = df_out.fillna(0)\n",
    "\n",
    "    #Zbior df_out do pierwszej klasyfikacji - klasa res_name\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def prepare_data(file, sep, use_cache = True):\n",
    "    prepared_data_patch = 'cache/prepared_data_'+file\n",
    "    if use_cache == True and os.path.isfile(prepared_data_patch):\n",
    "        df = joblib.load(prepared_data_patch)\n",
    "        return df\n",
    "    else:\n",
    "        df = pd.read_csv('data/'+file, sep=sep, na_values=[\"nan\"], low_memory=False, keep_default_na =False)\n",
    "        joblib.dump(df, prepared_data_patch)\n",
    "        return df\n",
    "\n",
    "def get_grouped_res_names(file):\n",
    "    #pobranie zgrupowanych etykiet\n",
    "    grouped_res_names = pd.read_csv(file,sep=\",\", na_values=[\"nan\"], low_memory=False, keep_default_na =False)\n",
    "    return grouped_res_names['res_name_group'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Próba użycia svd do redukcji wymiarów.\n",
    "W celu poprawy jakości klasyfikacji próbowaliśmy zastosować transformację SVD dla danych wejściowych. Na podstawie dokonanej wcześniej analizy wiedzieliśmy, że niektóre atrybuty są mocno skorelowane i chcieliśmy w ten sposób ograniczyć ich liczbę. \n",
    "Aby dokonać transformację SVD należało najpierw przetransformować wszystkie atrybuty na wartości dodatnie. W tym celu dokonaliśmy normalizacji wszystkich atrybutów do przedziału wartości 0 - 1.\n",
    "Dodatkowo należało usunąć z danych wszystkie wartości NA i NaN. Zrealizowaliśmy to poprzez ustawienie tych wartości na bardzo dużą liczbę.\n",
    "\n",
    "Na tak przetworzonych danych zastosowaliśmy transformację SVD i wybraliśmy pierwsze noOfNewParams najistotnijszych parametrów.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def use_svd(df, noOfNewParams = 100, NA_fallback_value = 10000):\n",
    "    LA = np.linalg\n",
    "\n",
    "    x = df.select_dtypes(include=['float64','int64'])\n",
    "\n",
    "    dfNormalized = (x - x.mean()) / (x.max() - x.min())\n",
    "    dfNormalized = dfNormalized - dfNormalized.min()\n",
    "    dfNormalized = dfNormalized.fillna(NA_fallback_value)\n",
    "\n",
    "    params = dfNormalized.select_dtypes(include=['float64','int64']).as_matrix()\n",
    "\n",
    "\n",
    "\n",
    "    U, s, Vh = LA.svd(params, full_matrices=False)\n",
    "    assert np.allclose(params, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "\n",
    "    s[noOfNewParams:] = 0\n",
    "\n",
    "    newParams = np.dot(U, np.diag(s))\n",
    "    newParams = pd.DataFrame(newParams[:,:noOfNewParams])\n",
    "    return newParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podczas testów okazało się, że taka transforamcja nie wpłyneła pozytywnie na jakość klasyfikacji. W ekserymencie uzyskaliśmy następujące wyniki:\n",
    "\n",
    "| no of parmas | recall         | n_estimators |\n",
    "|--------------|----------------|--------------|\n",
    "|           40 | 0.376368613139 |          180 |\n",
    "|           50 | 0.388686131387 |          140 |\n",
    "|           60 | 0.405109489051 |          180 |\n",
    "|           70 | 0.405565693431 |          180 |\n",
    "|           80 | 0.405565693431 |          180 |\n",
    "|           90 | 0.409671532847 |          180 |\n",
    "|          100 |  0.41697080292 |          160 |\n",
    "|          110 | 0.409215328467 |          180 |\n",
    "|          120 | 0.410583941606 |          180 |\n",
    "|          130 | 0.407390510949 |          160 |\n",
    "|          140 | 0.409671532847 |          180 |\n",
    "|          150 | 0.412408759124 |          180 |\n",
    "|          160 | 0.400547445255 |          140 |\n",
    "|          170 | 0.409671532847 |          180 |\n",
    "|          180 | 0.407390510949 |          180 |\n",
    "|--------------|----------------|--------------|\n",
    "| bez SVD: 755 | 0.458941605839 |          160 |\n",
    "\n",
    "\n",
    "Parametry testowe dla n_estimators wynosiły: np.arange(60, 300, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_df(df, grouped_res_names, column_name):\n",
    "    df[column_name] = grouped_res_names[column_name]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Uczenie klasyfikatora\n",
    "Do klasyfikacji wykorzystaliśmy algorytm RandomForest. Podczas uczenia poszukujemy najlepszego klasyfikatora przy pomocy parameter grid.\n",
    "Najlepszy estymator zapisujemy zgodnie z zaleceniami przy pomocy bilbioteki joblib. Niestety po zserializowaniu i zapisaniu do plików każdy z klasyfikatorów zajmuje blisko 2gb pamięci. Wynika to ze sporej ilości drzew w utworzonym klasyfikatorze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_estimator(df,classes,cache_file,use_cache = True):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, classes, test_size=0.2, random_state=0, stratify=classes)\n",
    "\n",
    "    score = 'recall_weighted'\n",
    "\n",
    "    if use_cache == True and os.path.isfile(cache_file):\n",
    "        best_estimator = joblib.load(cache_file)\n",
    "    else:\n",
    "        param_grid = {\n",
    "            'n_estimators': np.arange(100, 200, 20),\n",
    "            'max_features': ['sqrt'\n",
    "                             , 'log2'\n",
    "                            ]\n",
    "        }\n",
    "\n",
    "        print(\"starting classification: (\",time.ctime(),')')\n",
    "        rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True, warm_start=False, random_state=RANDOM_STATE)\n",
    "        \n",
    "        clf = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=2, scoring=score)\n",
    "        clf.fit(X_train, y_train)\n",
    "        #zapisanie klasyfikatora\n",
    "        best_estimator = clf.best_estimator_\n",
    "        joblib.dump(best_estimator, cache_file)\n",
    "\n",
    "    y_true, y_pred = y_test, best_estimator.predict(X_test)\n",
    "    print(cache_file)\n",
    "    print(best_estimator.get_params())\n",
    "    # print(classification_report(y_true, y_pred))\n",
    "    print(score, recall_score(y_true, y_pred))\n",
    "    return best_estimator\n",
    "\n",
    "    return best_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Klasyfikacja\n",
    "Wykorzystujemy wyżej zaimplementowane funkcje do przeprowadzenia klasyfikacji.\n",
    "Po wczytaniu danych testowych ograniczamy je do zbioru kolumn identycznego ze zbiorem treningowym.\n",
    "Wektory wynikowe dla obu klasyfikatorów zapisywane są następnie do plików w katalogu output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape: (11005, 756)\n",
      "train_data.shape after drop('res_name'): (11005, 755)\n",
      "test_data.shape: (18917, 755)\n",
      "cache/clf_for_orig_res_names_all_params\n",
      "{'n_jobs': -1, 'bootstrap': True, 'max_leaf_nodes': None, 'oob_score': True, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'n_estimators': 180, 'max_depth': None, 'warm_start': False, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'class_weight': None, 'criterion': 'gini', 'verbose': 0, 'random_state': 42}\n",
      "recall_weighted 0.405393053016\n",
      "cache/clf_for_grouped_res_names_all_params"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Software\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "E:\\Software\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'n_jobs': -1, 'bootstrap': True, 'max_leaf_nodes': None, 'oob_score': True, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'n_estimators': 180, 'max_depth': None, 'warm_start': False, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 2, 'class_weight': None, 'criterion': 'gini', 'verbose': 0, 'random_state': 42}\n",
      "recall_weighted 0.458941605839\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for dir in ['cache', 'output']:\n",
    "        if not os.path.exists(dir):\n",
    "            os.mkdir(dir)\n",
    "\n",
    "    train_data =  preprocess_data(prepare_data('all_summary.txt',';'))\n",
    "    print(\"train_data.shape:\", train_data.shape)\n",
    "\n",
    "    orig_res_names = train_data['res_name'].as_matrix()\n",
    "    grouped_res_names = get_grouped_res_names(\"data/grouped_res_name.txt\")\n",
    "\n",
    "    train_data = train_data.drop('res_name',axis=1)\n",
    "    print(\"train_data.shape after drop('res_name'):\", train_data.shape)\n",
    "\n",
    "    test_data = prepare_data('test_data.txt',',')\n",
    "    test_data = test_data.loc[:,train_data.columns]\n",
    "    test_data = test_data.fillna(0)\n",
    "\n",
    "    print(\"test_data.shape:\", test_data.shape)\n",
    "\n",
    "    # orig_res_names:\n",
    "\n",
    "    orig_estimator = get_estimator(train_data,orig_res_names,'cache/clf_for_orig_res_names_all_params',use_cache = True)\n",
    "\n",
    "    orig_test_predict = orig_estimator.predict(test_data)\n",
    "    np.savetxt(\"output/orig_test_predict.csv\", orig_test_predict, delimiter=\",\", fmt=\"%s\")\n",
    "\n",
    "    # for i in np.arange(40, 200, 20):\n",
    "    #     cache_file = 'cache/clf_for_orig_res_names_svd_'+str(i)+'_params'\n",
    "    #     df_svd = use_svd(df, noOfNewParams = i, NA_fallback_value = 10000)\n",
    "    #     classify_data(df_svd,orig_res_names,i,cache_file,)\n",
    "\n",
    "    # grouped_res_names:\n",
    "\n",
    "    grouped_estimator = get_estimator(train_data,grouped_res_names,'cache/clf_for_grouped_res_names_all_params',use_cache = True)\n",
    "\n",
    "    grouped_test_predict = grouped_estimator.predict(test_data)\n",
    "    np.savetxt(\"output/grouped_test_predict\", grouped_test_predict, delimiter=\",\", fmt=\"%s\")\n",
    "\n",
    "    # for i in np.arange(40, 200, 20):\n",
    "    #     cache_file = 'cache/clf_for_grouped_res_names_svd_'+str(i)+'_params'\n",
    "    #     df_svd = use_svd(df, noOfNewParams = i, NA_fallback_value = 10000)\n",
    "    #     classify_data(df_svd,grouped_res_names,i,cache_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Podsumowanie\n",
    "Musimy przyznać, że spodziewaliśmy się lepszych wyników przy zastosowaniu SVD. Uzyskana dokładność klasyfikacji wydaje się nam dosyć niska, jednak jest zrozumiała, ponieważ była to klasyfikacja wieloklasowa.\n",
    "\n",
    "Algorytm Random Forest i tak spisał się lepiej niż wypróbowany przez nas wcześniej algorytm SVM, który dawał o ponad 20% gorszą jakość klasyfikacji."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
